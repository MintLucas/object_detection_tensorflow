{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import time \n",
    "import sys\n",
    "import argparse \n",
    "import multiprocessing \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib # Matplotlib chooses Xwindows backend by default. matplotlib.use('Agg') \n",
    "from object_detection.utils import label_map_util \n",
    "from object_detection.utils import visualization_utils as vis_util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'ssd_mobilenet_v1_coco_2018_01_28' \n",
    "PATH_TO_CKPT = os.path.join(MODEL_NAME, 'frozen_inference_graph.pb') # List of the strings that is used to add correct label for each box. \n",
    "PATH_TO_LABELS = os.path.join('data', 'final_label_map.pbtxt')\n",
    "NUM_CLASSES = 2 \n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS) \n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True) \n",
    "category_index = label_map_util.create_category_index(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_objects(image_np, sess, detection_graph): \n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3] \n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    #loader = tf.train.import_meta_graph(model_path+'.meta')\n",
    "    #loader.restore(sess,model_path)\n",
    "    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "    boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "\n",
    "    scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "    classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "    (boxes, scores, classes, num_detections) = sess.run(\n",
    "        [boxes, scores, classes, num_detections],\n",
    "        feed_dict={image_tensor: image_np_expanded})\n",
    "\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np, \n",
    "        np.squeeze(boxes),\n",
    "        np.squeeze(classes).astype(np.int32), \n",
    "        np.squeeze(scores),\n",
    "        category_index, \n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=8)\n",
    "\n",
    "    return image_np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load a frozen TF model \n",
    "detection_graph = tf.Graph() \n",
    "with detection_graph.as_default(): \n",
    "    od_graph_def = tf.GraphDef() \n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid: \n",
    "        serialized_graph = fid.read() \n",
    "        od_graph_def.ParseFromString(serialized_graph) \n",
    "        tf.import_graph_def(od_graph_def, name='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def process_image(image):\n",
    "        # NOTE: The output you return should be a color image (3 channel) for processing video below \n",
    "        # you should return the final output (image with lines are drawn on lanes) \n",
    "        with detection_graph.as_default(): \n",
    "            with tf.Session(graph=detection_graph) as sess: \n",
    "                image_process = detect_objects(image, sess, detection_graph) \n",
    "                return image_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video video1_out.mp4\n",
      "[MoviePy] Writing video video1_out.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████████████████████████████████████████████▊  | 36/37 [02:50<00:04,  4.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: video1_out.mp4 \n",
      "\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "# 1.使用 VideoFileClip 函数从视频中抓取图片。 \n",
    "# 2.用fl_image函数将原图片替换为修改后的图片，用于传递物体识别的每张抓取图片。 \n",
    "# 3.所有修改的剪辑图像被组合成为一个新的视频。\n",
    "\n",
    "white_output = 'video1_out.mp4'\n",
    "clip1 = VideoFileClip(\"video1_clip.mp4\").subclip(5,8)\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!s\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"video1_out.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "#clip1 = VideoFileClip(\"video1_out.mp4\")\n",
    "#clip1.write_gif(\"final.gif\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
